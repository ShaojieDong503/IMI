# -*- coding: utf-8 -*-
"""base_clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-C9qS8digIzKLAvS5k5-RG6eKFIS6Cod
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import plotly.express as px
import os
from pathlib import Path

input_dir = os.getenv('INPUT_DIR', '/input') 
output_dir = os.getenv('OUTPUT_DIR', '/output') 
interim_dir = os.path.join(output_dir, 'interim')

def ensure_dir(path):
    Path(interim_dir).mkdir(parents=True, exist_ok=True)

new_general_table_path = os.path.join(interim_dir, 'new_general_table.csv')
card_path = os.path.join(input_dir, 'card.csv')

df = pd.read_csv(new_general_table_path)
df_card = pd.read_csv(card_path)

df.columns

# All the avialiable feature that could be use for cluster model
feature_all = ['total_debit_amount_cad', 'total_credit_amount_cad',
       'debit_count', 'credit_count', 'transaction_frequency',
       'avg_transaction_interval_day', 'mode_transaction_interval_day',
       'mode_transaction_type', 'date_range', 'max_credit_transaction_amount',
       'avg_credit_transaction_amount', 'max_debit_transaction_amount',
       'avg_debit_transaction_amount', 'structuring_points_x', 'funnel_index',
       'funnel_points', 'structuring_points_y', 'score_missing_kyc']

# Check the for all the feature above before building the model


correlation_matrix = df[feature_all].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)

plt.title('Correlation Matrix Heatmap', fontsize=16)


plt.show()

# This is all the final features we are going to use for clustering
# Removed: Total credit amount,debit_count, credit_count,max_credit_transaction_amount,max_debit_transaction_amount,structuring_points_x,count_index,funnel_points 'mode_transaction_type', 'date_range',

feature_final = ['total_debit_amount_cad',
        'transaction_frequency',
       'avg_transaction_interval_day', 'mode_transaction_interval_day',
       'avg_credit_transaction_amount',
       'avg_debit_transaction_amount',
       'funnel_points', 'structuring_points_y', 'score_missing_kyc']

correlation_matrix = df[feature_final].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', square=True)

plt.title('Correlation Matrix Heatmap', fontsize=16)


plt.show()

# Scale the features
scaler = StandardScaler()
df_final = df[feature_final]
data_scaled = scaler.fit_transform(df_final)
df_scaled = pd.DataFrame(data_scaled, columns=feature_final)

# Build a KMeans Model

silhouette_scores = []
k_range = range(2, 10)


inertia = []
silhouette_scores = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    clusters = kmeans.fit_predict(df_scaled)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(df_scaled, clusters))


plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(k_range, inertia, marker='o', linestyle='--', color='blue')
plt.xticks(k_range)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal Clusters')


plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, marker='o', linestyle='--', color='green')
plt.xticks(k_range)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Method for Optimal Clusters')

plt.tight_layout()
plt.show()

"""The rate of decrease in inertia slows significantly after k=3"""

# Use the Best K for the model
chosen_k = k_range[np.argmax(silhouette_scores)]
print(chosen_k)

kmeans = KMeans(n_clusters=chosen_k , random_state=71)
clusters = kmeans.fit_predict(df_scaled)
df['cluster'] = clusters

# Create an interactive scatter plot
fig = px.scatter(
    df,
    x='cluster',
    y='avg_debit_transaction_amount',
    color='cluster',  # Color points by their cluster,
    title='Interactive K-Means Clustering Visualization',
)

# Show the interactive plot
fig.show()

# Create a stacked bar plot
fig = px.histogram(
    df,
    x='cluster',
    y='funnel_points',
    color='cluster'  # Color bars by their cluster
)

# Show the plot
fig.show()

df.columns

"""以下是输出最后的算分和输出output file 1

--------------------------------------------------------------------------------------------------------------------------
"""

# Calculate the one with high transaction frequency
def assign_high_freq_score(
    df: pd.DataFrame,
    cluster_col: str = "cluster",
    freq_col: str = "transaction_frequency",
    score_col: str = "score_high_freq"
) -> pd.DataFrame:

    freq_95 = df.groupby(cluster_col)[freq_col].transform(lambda x: x.quantile(0.95))
    freq_99 = df.groupby(cluster_col)[freq_col].transform(lambda x: x.quantile(0.99))

    # Initialize or reset the score column to 0
    df[score_col] = 0

    # Assign 2 points for > 99th percentile
    mask_99 = df[freq_col] > freq_99
    df.loc[mask_99, score_col] = 1

    # Assign 1 point for > 95th percentile (but <= 99th)
    mask_95 = (df[freq_col] > freq_95) & (~mask_99)
    df.loc[mask_95, score_col] = 0.5

    return df

df = assign_high_freq_score(
    df,
    cluster_col="cluster",
    freq_col="transaction_frequency",
    score_col="score_high_freq"
)

# Calculate the one with high transaction debit amount
def assign_high_amount_score(
    df: pd.DataFrame,
    cluster_col: str = "cluster",
    amount_col: str = "total_debit_amount_cad",
    score_col: str = "score_debit_high_amount"
) -> pd.DataFrame:

    amount_95 = df.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.95))
    amount_99 = df.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.99))

    # Initialize or reset the score column to 0
    df[score_col] = 0

    # Assign 2 points for > 99th percentile
    mask_99 = df[amount_col] > amount_99
    df.loc[mask_99, score_col] = 1

    # Assign 1 point for > 95th percentile (but <= 99th)
    mask_95 = (df[amount_col] > amount_95) & (~mask_99)
    df.loc[mask_95, score_col] = 0.5

    return df

df = assign_high_amount_score(
    df,
    cluster_col="cluster",
    amount_col="total_debit_amount_cad",
    score_col="score_debit_high_amount"
)

# Calculate the one with high transaction credit amount
def assign_high_amount_score(
    df: pd.DataFrame,
    cluster_col: str = "cluster",
    amount_col: str = "total_credit_amount_cad",
    score_col: str = "score_credit_high_amount"
) -> pd.DataFrame:

    amount_95 = df.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.95))
    amount_99 = df.groupby(cluster_col)[amount_col].transform(lambda x: x.quantile(0.99))

    # Initialize or reset the score column to 0
    df[score_col] = 0

    # Assign 2 points for > 99th percentile
    mask_99 = df[amount_col] > amount_99
    df.loc[mask_99, score_col] = 1

    # Assign 1 point for > 95th percentile (but <= 99th)
    mask_95 = (df[amount_col] > amount_95) & (~mask_99)
    df.loc[mask_95, score_col] = 0.5

    return df

df = assign_high_amount_score(
    df,
    cluster_col="cluster",
    amount_col="total_credit_amount_cad",
    score_col="score_credit_high_amount"
)

df["score_total"] = df["funnel_points"] + df["structuring_points_y"]+df["score_missing_kyc"]+df["score_high_freq"]+df["score_debit_high_amount"]+df["score_credit_high_amount"]

# Persons with highest score(>99%) are flaged as bad actors
threshold = df["score_total"].quantile(0.995)
df["bad_actor"] = df["score_total"] > threshold.astype(int)


task1_output_path = os.path.join(output_dir, 'task1.csv')
ensure_dir(task1_output_path)
df.to_csv(task1_output_path, index=False)

df['bad_actor'].value_counts()

bad_actors_df = df[df['bad_actor'] == 1]
print(bad_actors_df)